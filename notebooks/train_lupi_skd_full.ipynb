{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LUPI-SKD Full Training Notebook\n",
    "\n",
    "## Overview\n",
    "- **Goal**: Train Qwen3-4B with Korean cultural knowledge using LUPI-SKD\n",
    "- **Teacher**: Qwen3-32B (sees Q+K)\n",
    "- **Student**: Qwen3-4B-Thinking (sees Q only)\n",
    "- **Data**: korean_culture_train_200.json (180 train / 20 val)\n",
    "\n",
    "## Key Settings\n",
    "- Top-K: 5 (fixed)\n",
    "- Epochs: 3\n",
    "- LR: 2e-6\n",
    "- Grad Accum: 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/eoeldroal/WorkPlace/SKD_RAG_K\n",
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA available: True\n",
      "CUDA devices: 3\n",
      "\n",
      "[Rollout Functions]\n",
      "  Default: lupi_skd_rollout_optimized (stable)\n",
      "  Available: lupi_skd_rollout_compiled (experimental)\n",
      "  Original: lupi_skd_rollout_original (for A/B testing)\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Environment Setup\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "# CUDA setup\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5,6,7\"\n",
    "\n",
    "# ML imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, get_scheduler\n",
    "\n",
    "# Logging\n",
    "import wandb\n",
    "from tqdm.auto import tqdm  # auto selects best backend\n",
    "\n",
    "# Project root (relative to notebook location)\n",
    "PROJECT_ROOT = Path.cwd().parent  # notebooks/ -> SKD_RAG_K/\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# Import optimized rollout functions (v2)\n",
    "from train.lupi_rollout_optimized import lupi_skd_rollout_optimized\n",
    "# from train.lupi_rollout_compiled import lupi_skd_rollout_compiled\n",
    "\n",
    "# Default to stable version (can switch to compiled for testing)\n",
    "lupi_skd_rollout = lupi_skd_rollout_optimized\n",
    "# lupi_skd_rollout = lupi_skd_rollout_compiled\n",
    "\n",
    "# Keep original for A/B comparison\n",
    "from train.lupi_rollout import lupi_skd_rollout_block as lupi_skd_rollout_original\n",
    "\n",
    "# Seed for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA devices: {torch.cuda.device_count()}\")\n",
    "\n",
    "print(f\"\\n[Rollout Functions]\")\n",
    "print(f\"  Default: lupi_skd_rollout_optimized (stable)\")\n",
    "print(f\"  Available: lupi_skd_rollout_compiled (experimental)\")\n",
    "print(f\"  Original: lupi_skd_rollout_original (for A/B testing)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Configuration]\n",
      "  Teacher: Qwen/Qwen3-32B\n",
      "  Student: Qwen/Qwen3-4B-Thinking-2507\n",
      "  Data path: /home/eoeldroal/WorkPlace/SKD_RAG_K/data/korean_culture_train_200.json\n",
      "  Checkpoint dir: /home/eoeldroal/WorkPlace/SKD_RAG_K/checkpoints/lupi_skd\n",
      "  Top-K: 15\n",
      "  Epochs: 3\n",
      "  LR: 2e-06\n",
      "  Grad Accum: 4\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Configuration Dataclass\n",
    "\n",
    "@dataclass\n",
    "class LUPISKDConfig:\n",
    "    # Model paths\n",
    "    teacher_model_name: str = \"Qwen/Qwen3-32B\"\n",
    "    student_model_name: str = \"Qwen/Qwen3-4B-Thinking-2507\"\n",
    "    \n",
    "    # Data (use PROJECT_ROOT from Cell 1)\n",
    "    data_path: str = str(PROJECT_ROOT / \"data/korean_culture_train_200.json\")\n",
    "    train_val_split: float = 0.9  # 180 train, 20 val\n",
    "    \n",
    "    # SKD hyperparameters\n",
    "    top_k: int = 15\n",
    "    gamma: int = 5\n",
    "    teacher_temperature: float = 0.7\n",
    "    student_temperature: float = 0.7\n",
    "    teacher_top_p: float = 0.95\n",
    "    student_top_p: float = 0.95\n",
    "    \n",
    "    # Training\n",
    "    epochs: int = 3\n",
    "    learning_rate: float = 2e-6\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    max_grad_norm: float = 1.0\n",
    "    max_new_tokens: int = 8192\n",
    "    warmup_ratio: float = 0.1\n",
    "    \n",
    "    # Checkpoint (use PROJECT_ROOT from Cell 1)\n",
    "    checkpoint_dir: str = str(PROJECT_ROOT / \"checkpoints/lupi_skd\")\n",
    "    save_every_epoch: bool = True\n",
    "    \n",
    "    # WandB\n",
    "    wandb_project: str = \"LUPI-SKD-Korean-Culture\"\n",
    "    wandb_run_name: str = None  # Auto-generated if None\n",
    "    \n",
    "    # Misc\n",
    "    seed: int = 42\n",
    "    verbose: bool = True\n",
    "    log_every_steps: int = 5\n",
    "    val_every_steps: int = 45  # Roughly once per epoch (180/4=45 steps)\n",
    "\n",
    "config = LUPISKDConfig()\n",
    "\n",
    "print(\"[Configuration]\")\n",
    "print(f\"  Teacher: {config.teacher_model_name}\")\n",
    "print(f\"  Student: {config.student_model_name}\")\n",
    "print(f\"  Data path: {config.data_path}\")\n",
    "print(f\"  Checkpoint dir: {config.checkpoint_dir}\")\n",
    "print(f\"  Top-K: {config.top_k}\")\n",
    "print(f\"  Epochs: {config.epochs}\")\n",
    "print(f\"  LR: {config.learning_rate}\")\n",
    "print(f\"  Grad Accum: {config.gradient_accumulation_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Teacher: Qwen/Qwen3-32B\n",
      "Loading Student: Qwen/Qwen3-4B-Thinking-2507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7387a9e07c5e4e1f912a23ce20451fdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e8fc90df1a041a5b89e6f004db72c2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e46423db36e8417fa7575c00151addfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0fde3c16e6649f3a10486d618aa55b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd068122c2924a1b95dc98deea7e746c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99148287f4634554bd16b531046c02eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Models Loaded]\n",
      "  Teacher params: 32.76B\n",
      "  Student params: 4.02B\n",
      "  Student trainable: 4.02B\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Model Loading\n",
    "\n",
    "print(f\"Loading Teacher: {config.teacher_model_name}\")\n",
    "print(f\"Loading Student: {config.student_model_name}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.teacher_model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Teacher: eval mode, no gradient\n",
    "teacher_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.teacher_model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"kernels-community/vllm-flash-attn3\",\n",
    ").eval()\n",
    "\n",
    "for param in teacher_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Student: train mode\n",
    "student_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.student_model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"kernels-community/vllm-flash-attn3\",\n",
    ")\n",
    "\n",
    "print(f\"\\n[Models Loaded]\")\n",
    "print(f\"  Teacher params: {sum(p.numel() for p in teacher_model.parameters())/1e9:.2f}B\")\n",
    "print(f\"  Student params: {sum(p.numel() for p in student_model.parameters())/1e9:.2f}B\")\n",
    "print(f\"  Student trainable: {sum(p.numel() for p in student_model.parameters() if p.requires_grad)/1e9:.2f}B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Data Loaded]\n",
      "  Total samples: 200\n",
      "  Train samples: 180\n",
      "  Val samples: 20\n",
      "\n",
      "  Train category distribution:\n",
      "    ì—­ì‚¬/ì¸ë¬¼: 35\n",
      "    ìŒì‹/ë°œíš¨: 39\n",
      "    ì „í†µì˜ˆì ˆ: 32\n",
      "    ì§€ë¦¬/ì§€ì—­: 38\n",
      "    í˜„ëŒ€ë¬¸í™”: 36\n",
      "\n",
      "[Sample Data]\n",
      "  ID: history_027\n",
      "  Category: ì—­ì‚¬/ì¸ë¬¼\n",
      "  Query: 1895ë…„ ì„ë¯¸ì‚¬ë³€ ì´í›„ ì„ë¯¸ì˜ë³‘ì´ ì¼ì–´ë‚œ ê²½ìœ„ì™€ ì£¼ìš” ì˜ë³‘ìž¥ë“¤ì˜ í™œë™ì€ ì–´ë– í–ˆë‚˜ìš”?...\n",
      "  Knowledge: ì„ë¯¸ì˜ë³‘ì€ 1895ë…„(ì„ë¯¸ë…„) ì„ë¯¸ì‚¬ë³€ê³¼ ë‹¨ë°œë ¹ì— í•­ê±°í•˜ì—¬ ì¼ì–´ë‚œ ì˜ë³‘ ìš´ë™ì´ë‹¤. ì„ë¯¸ì‚¬ë³€ì€ 1895ë…„ 10ì›” 8ì¼(ìŒë ¥ 8ì›” 20ì¼) ì¼ë³¸ ê³µì‚¬ ë¯¸ìš°ë¼ ê³ ë¡œì˜ ì§€íœ˜ ì•„ëž˜ ì¼ë³¸êµ°ì´ ê²½ë³µê¶ì— ë‚œìž…í•˜ì—¬ ëª…ì„±í™©í›„(ë¯¼ë¹„)ë¥¼ ì‹œí•´í•œ ì‚¬ê±´ì´ë‹¤. ì´ ì†Œì‹ì´ ì•Œë ¤ì§€ë©´ì„œ êµ­ë¯¼ì  ë¶„ë…¸ê°€ í­ë°œí•˜ì˜€ê³ , ê¹€í™ì§‘ ì¹œì¼ ë‚´ê°ì´ 11ì›” ë‹¨ë°œë ¹ì„ ì‹œí–‰í•˜ìž ìœ ìƒë“¤ì´ ì˜ë³‘ì„ ì¼ìœ¼ì¼°ë‹¤. ì¶©ì²­ë‚¨ë„ì—ì„œ ë¬¸ì„ë´‰ì´ 9ì›” 18ì¼ ìµœì´ˆë¡œ ì˜ë³‘ì„ ë´‰ê¸°í•˜ì˜€ê³ , 11ì›” ì¶©ì²­ë„ ì œì²œì—ì„œëŠ” ìœ ì¸ì„ì´ 'ê±°ì˜í† ì (æ“§ç¾©è¨Žè³Š)'ì˜ ê¸°ì¹˜ë¥¼ ì˜¬ë¦¬ë©° ì˜ë³‘ì„ ì¡°ì§í•˜ì˜€ë‹¤. ê°•ì›ë„ ê°•ë¦‰ì—ì„œëŠ” ë¯¼ìš©í˜¸ê°€ 1896ë…„ 1ì›” 30ì¼ ê´€ë™9êµ°ë„ì°½ì˜ì†Œë¥¼ ì„¤ì¹˜í•˜ê³  ê°•ë¦‰ë¶€ ê²½ë¬´ê´€ ê³ ì¤€ì„ì„ ì²˜ë‹¨í•˜ì˜€ë‹¤. ì˜ë³‘ë“¤ì€ ì§€ë°© ë„ì‹œë¥¼ ê³µëžµí•˜ì—¬ ì¹œì¼ ê´€ë¦¬ì™€ ì¼ë³¸ì¸ì„ ì²˜ë‹¨í•˜ì˜€ìœ¼ë©°, ì˜ë³‘ ìˆ˜ëŠ” ìµœê³  5ë§Œ ëª…ì— ë‹¬í•˜ì˜€ë‹¤. ê·¸ëŸ¬ë‚˜ 1896ë…„ 2ì›” ê³ ì¢…ì´ ëŸ¬ì‹œì•„ ê³µì‚¬ê´€ìœ¼ë¡œ í”¼ì‹ í•˜ëŠ” ì•„ê´€íŒŒì²œ ì´í›„, ê³ ì¢…ì´ ì˜ë³‘ í•´ì‚°ì„ ê¶Œê³ í•˜ëŠ” ì¡°ì¹™ì„ ë‚´ë¦¬ìž ì˜ë³‘ë“¤ì€ ìžì§„ í•´ì‚°í•˜ì˜€ë‹¤. ì„ë¯¸ì˜ë³‘ì€ ë¯¼ì¡±ì˜ ìžì£¼ì„±ì„ ì§€í‚¤ë ¤ëŠ” ìžë°œì  ë¬´ìž¥ í•­ìŸì´ì—ˆìœ¼ë©°, ì´í›„ ì„ì‚¬ì˜ë³‘, ì •ë¯¸ì˜ë³‘ìœ¼ë¡œ ì´ì–´ì¡Œë‹¤....\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Dataset Loading and Splitting\n",
    "\n",
    "def load_and_split_data(config: LUPISKDConfig):\n",
    "    \"\"\"Load korean_culture_train_200.json and split 9:1.\"\"\"\n",
    "    with open(config.data_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Shuffle with fixed seed\n",
    "    random.seed(config.seed)\n",
    "    random.shuffle(data)\n",
    "    \n",
    "    split_idx = int(len(data) * config.train_val_split)\n",
    "    train_data = data[:split_idx]  # 180 samples\n",
    "    val_data = data[split_idx:]    # 20 samples\n",
    "    \n",
    "    print(f\"[Data Loaded]\")\n",
    "    print(f\"  Total samples: {len(data)}\")\n",
    "    print(f\"  Train samples: {len(train_data)}\")\n",
    "    print(f\"  Val samples: {len(val_data)}\")\n",
    "    \n",
    "    # Category distribution\n",
    "    train_cats = {}\n",
    "    for sample in train_data:\n",
    "        cat = sample.get('category', 'unknown')\n",
    "        train_cats[cat] = train_cats.get(cat, 0) + 1\n",
    "    print(f\"\\n  Train category distribution:\")\n",
    "    for cat, count in sorted(train_cats.items()):\n",
    "        print(f\"    {cat}: {count}\")\n",
    "    \n",
    "    return train_data, val_data\n",
    "\n",
    "train_data, val_data = load_and_split_data(config)\n",
    "\n",
    "# Show sample\n",
    "print(f\"\\n[Sample Data]\")\n",
    "sample = train_data[0]\n",
    "print(f\"  ID: {sample.get('id', 'N/A')}\")\n",
    "print(f\"  Category: {sample.get('category', 'N/A')}\")\n",
    "print(f\"  Query: {sample['query']}...\")\n",
    "print(f\"  Knowledge: {sample['knowledge']}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LUPI Input Test]\n",
      "  Teacher input tokens: 468\n",
      "  Student input tokens: 47\n",
      "  Knowledge tokens (diff): 421\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: LUPI Input Builder (from test_lupi_skd.ipynb)\n",
    "\n",
    "def build_lupi_inputs(query: str, knowledge: str, tokenizer):\n",
    "    \"\"\"Build LUPI inputs: Teacher sees Q+K, Student sees Q only.\"\"\"\n",
    "    teacher_messages = [\n",
    "        {\"role\": \"system\", \"content\": knowledge},\n",
    "        {\"role\": \"user\", \"content\": query},\n",
    "    ]\n",
    "    student_messages = [\n",
    "        {\"role\": \"user\", \"content\": query},\n",
    "    ]\n",
    "\n",
    "    teacher_inputs = tokenizer.apply_chat_template(\n",
    "        teacher_messages,\n",
    "        return_dict=True,\n",
    "        tokenize=True,\n",
    "        return_tensors=\"pt\",\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=True,\n",
    "    )\n",
    "    student_inputs = tokenizer.apply_chat_template(\n",
    "        student_messages,\n",
    "        return_dict=True,\n",
    "        tokenize=True,\n",
    "        return_tensors=\"pt\",\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=True,\n",
    "    )\n",
    "    return teacher_inputs, student_inputs\n",
    "\n",
    "# Test\n",
    "test_t, test_s = build_lupi_inputs(train_data[0]['query'], train_data[0]['knowledge'], tokenizer)\n",
    "print(f\"[LUPI Input Test]\")\n",
    "print(f\"  Teacher input tokens: {test_t['input_ids'].shape[1]}\")\n",
    "print(f\"  Student input tokens: {test_s['input_ids'].shape[1]}\")\n",
    "print(f\"  Knowledge tokens (diff): {test_t['input_ids'].shape[1] - test_s['input_ids'].shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸ” OPTIMIZED LUPI-SKD ROLLOUT DEBUG\n",
      "======================================================================\n",
      "\n",
      "[1] ìƒ˜í”Œ ì •ë³´\n",
      "    ID: history_027\n",
      "    Category: ì—­ì‚¬/ì¸ë¬¼\n",
      "    Query ê¸¸ì´: 47 chars\n",
      "    Knowledge ê¸¸ì´: 541 chars\n",
      "\n",
      "[2] í† í°í™”ëœ ìž…ë ¥ ë¹„êµ\n",
      "    Teacher input tokens: 468\n",
      "    Student input tokens: 47\n",
      "    ì°¨ì´ (Knowledge tokens): 421\n",
      "\n",
      "======================================================================\n",
      "[3] ðŸŽï¸ A/B SPEED COMPARISON (Original vs Optimized)\n",
      "======================================================================\n",
      "\n",
      "  [A] ORIGINAL (lupi_skd_rollout_original)\n",
      "      max_new_tokens: 1024\n"
     ]
    }
   ],
   "source": [
    "# Cell 5.5: ðŸ” OPTIMIZED ROLLOUT DEBUG & A/B COMPARISON\n",
    "# ëª©ì : 1) ìµœì í™”ëœ rollout í…ŒìŠ¤íŠ¸, 2) ê¸°ì¡´ ë²„ì „ê³¼ ì†ë„ ë¹„êµ, 3) íŒŒì´í”„ë¼ì¸ ê²€ì¦\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸ” OPTIMIZED LUPI-SKD ROLLOUT DEBUG\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================\n",
    "# 1. ìƒ˜í”Œ ì„ íƒ ë° í”„ë¡¬í”„íŠ¸ êµ¬ë¶„ ê²€ì¦\n",
    "# ============================================================\n",
    "debug_sample = train_data[0]\n",
    "print(f\"\\n[1] ìƒ˜í”Œ ì •ë³´\")\n",
    "print(f\"    ID: {debug_sample.get('id', 'N/A')}\")\n",
    "print(f\"    Category: {debug_sample.get('category', 'N/A')}\")\n",
    "print(f\"    Query ê¸¸ì´: {len(debug_sample['query'])} chars\")\n",
    "print(f\"    Knowledge ê¸¸ì´: {len(debug_sample['knowledge'])} chars\")\n",
    "\n",
    "# LUPI ìž…ë ¥ ìƒì„±\n",
    "teacher_inputs, student_inputs = build_lupi_inputs(\n",
    "    debug_sample['query'], debug_sample['knowledge'], tokenizer\n",
    ")\n",
    "\n",
    "print(f\"\\n[2] í† í°í™”ëœ ìž…ë ¥ ë¹„êµ\")\n",
    "print(f\"    Teacher input tokens: {teacher_inputs['input_ids'].shape[1]}\")\n",
    "print(f\"    Student input tokens: {student_inputs['input_ids'].shape[1]}\")\n",
    "print(f\"    ì°¨ì´ (Knowledge tokens): {teacher_inputs['input_ids'].shape[1] - student_inputs['input_ids'].shape[1]}\")\n",
    "\n",
    "# ============================================================\n",
    "# 2. A/B ì†ë„ ë¹„êµ í…ŒìŠ¤íŠ¸\n",
    "# ============================================================\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"[3] ðŸŽï¸ A/B SPEED COMPARISON (Original vs Optimized)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "DEBUG_MAX_TOKENS = 1024  # ì†ë„ ë¹„êµìš©\n",
    "\n",
    "# --- A) Original Version ---\n",
    "print(f\"\\n  [A] ORIGINAL (lupi_skd_rollout_original)\")\n",
    "print(f\"      max_new_tokens: {DEBUG_MAX_TOKENS}\")\n",
    "torch.cuda.synchronize()\n",
    "start_original = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    _, _, tokens_original, text_original = lupi_skd_rollout_original(\n",
    "        teacher_model=teacher_model,\n",
    "        student_model=student_model,\n",
    "        teacher_input_ids=teacher_inputs['input_ids'],\n",
    "        student_input_ids=student_inputs['input_ids'],\n",
    "        max_new_tokens=DEBUG_MAX_TOKENS,\n",
    "        top_k=config.top_k,\n",
    "        gamma=config.gamma,\n",
    "        teacher_temperature=config.teacher_temperature,\n",
    "        student_temperature=config.student_temperature,\n",
    "        teacher_top_p=config.teacher_top_p,\n",
    "        student_top_p=config.student_top_p,\n",
    "        tokenizer=tokenizer,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "time_original = time.time() - start_original\n",
    "tokens_per_sec_original = len(tokens_original) / time_original\n",
    "\n",
    "print(f\"      Time: {time_original:.2f}s\")\n",
    "print(f\"      Tokens: {len(tokens_original)}\")\n",
    "print(f\"      Speed: {tokens_per_sec_original:.1f} tok/s\")\n",
    "\n",
    "# --- B) Optimized Version ---\n",
    "print(f\"\\n  [B] OPTIMIZED (lupi_skd_rollout_optimized)\")\n",
    "print(f\"      max_new_tokens: {DEBUG_MAX_TOKENS}\")\n",
    "torch.cuda.synchronize()\n",
    "start_optimized = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    _, _, tokens_optimized, text_optimized, debug_stats = lupi_skd_rollout_optimized(\n",
    "        teacher_model=teacher_model,\n",
    "        student_model=student_model,\n",
    "        teacher_input_ids=teacher_inputs['input_ids'],\n",
    "        student_input_ids=student_inputs['input_ids'],\n",
    "        max_new_tokens=DEBUG_MAX_TOKENS,\n",
    "        top_k=config.top_k,\n",
    "        gamma=config.gamma,\n",
    "        teacher_temperature=config.teacher_temperature,\n",
    "        student_temperature=config.student_temperature,\n",
    "        teacher_top_p=config.teacher_top_p,\n",
    "        student_top_p=config.student_top_p,\n",
    "        tokenizer=tokenizer,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "time_optimized = time.time() - start_optimized\n",
    "tokens_per_sec_optimized = len(tokens_optimized) / time_optimized\n",
    "\n",
    "print(f\"      Time: {time_optimized:.2f}s\")\n",
    "print(f\"      Tokens: {len(tokens_optimized)}\")\n",
    "print(f\"      Speed: {tokens_per_sec_optimized:.1f} tok/s\")\n",
    "\n",
    "# --- Comparison Summary ---\n",
    "speedup = time_original / time_optimized if time_optimized > 0 else 0\n",
    "print(f\"\\n  [COMPARISON SUMMARY]\")\n",
    "print(f\"      Speedup: {speedup:.2f}x {'âœ…' if speedup > 1.5 else 'âš ï¸'}\")\n",
    "print(f\"      Original: {tokens_per_sec_original:.1f} tok/s\")\n",
    "print(f\"      Optimized: {tokens_per_sec_optimized:.1f} tok/s\")\n",
    "print(f\"      Time saved: {time_original - time_optimized:.2f}s ({(1 - time_optimized/time_original)*100:.1f}%)\")\n",
    "\n",
    "# ============================================================\n",
    "# 3. ìµœì í™” ë²„ì „ ìƒì„¸ í†µê³„\n",
    "# ============================================================\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"[4] ðŸ“Š OPTIMIZED VERSION DETAILED STATS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"  Total blocks: {debug_stats['total_blocks']}\")\n",
    "print(f\"  Total drafted: {debug_stats['total_drafted']}\")\n",
    "print(f\"  Total accepted: {debug_stats['total_accepted']}\")\n",
    "print(f\"  Total rejected: {debug_stats['total_rejected']}\")\n",
    "print(f\"  Avg acceptance rate: {debug_stats['avg_acceptance_rate']:.1%}\")\n",
    "print(f\"  Teacher forward calls: {debug_stats['teacher_forward_calls']}\")\n",
    "print(f\"  Student forward calls: {debug_stats['student_forward_calls']}\")\n",
    "\n",
    "# ê¸°ì¡´ ë²„ì „ì´ë¼ë©´ Teacher forward = gamma * blocks ë²ˆ í˜¸ì¶œë˜ì—ˆì„ ê²ƒ\n",
    "estimated_original_teacher_calls = debug_stats['total_blocks'] * config.gamma\n",
    "print(f\"\\n  [Forward Call Comparison]\")\n",
    "print(f\"    Original (estimated): ~{estimated_original_teacher_calls} Teacher calls\")\n",
    "print(f\"    Optimized (actual): {debug_stats['teacher_forward_calls']} Teacher calls\")\n",
    "print(f\"    Reduction: {(1 - debug_stats['teacher_forward_calls']/estimated_original_teacher_calls)*100:.1f}%\")\n",
    "\n",
    "# Per-block acceptance rates\n",
    "if debug_stats['block_stats']:\n",
    "    print(f\"\\n  [Per-Block Acceptance Rates (first 10)]\")\n",
    "    for i, bs in enumerate(debug_stats['block_stats'][:10]):\n",
    "        bar = 'â–ˆ' * int(bs['acceptance_rate'] * 10) + 'â–‘' * (10 - int(bs['acceptance_rate'] * 10))\n",
    "        print(f\"    Block {i+1:2d}: {bar} {bs['n_matches']}/{bs['gamma']} ({bs['acceptance_rate']:.0%})\")\n",
    "\n",
    "# ============================================================\n",
    "# 4. ìƒì„±ëœ í…ìŠ¤íŠ¸ ë¹„êµ\n",
    "# ============================================================\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"[5] ðŸ“„ GENERATED TEXT COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n  [Original ({len(tokens_original)} tokens)]\")\n",
    "print(f\"  {text_original}\")\n",
    "\n",
    "print(f\"\\n  [Optimized ({len(tokens_optimized)} tokens)]\")\n",
    "print(f\"  {text_optimized}\")\n",
    "\n",
    "# Check if outputs are similar (they won't be identical due to sampling)\n",
    "print(f\"\\n  Note: Outputs differ due to stochastic sampling, but quality should be comparable.\")\n",
    "\n",
    "# ============================================================\n",
    "# 5. ê²€ì¦ ì™„ë£Œ\n",
    "# ============================================================\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"âœ… DEBUG COMPLETE - ìµœì í™” ê²€ì¦ ì™„ë£Œ!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nìš”ì•½:\")\n",
    "print(f\"  - ì†ë„ í–¥ìƒ: {speedup:.2f}x {'âœ… ì„±ê³µ' if speedup > 1.5 else 'âš ï¸ ì¶”ê°€ í™•ì¸ í•„ìš”'}\")\n",
    "print(f\"  - Teacher forward ê°ì†Œ: {(1 - debug_stats['teacher_forward_calls']/estimated_original_teacher_calls)*100:.1f}%\")\n",
    "print(f\"  - Acceptance Rate: {debug_stats['avg_acceptance_rate']:.1%}\")\n",
    "print(f\"  - íŒŒì´í”„ë¼ì¸ ë™ìž‘: âœ… ({len(tokens_optimized)} tokens ìƒì„±)\")\n",
    "\n",
    "# ê¸€ë¡œë²Œ rollout í•¨ìˆ˜ë¥¼ ìµœì í™” ë²„ì „ìœ¼ë¡œ í™•ì •\n",
    "if speedup > 1.0:\n",
    "    print(f\"\\n  âœ… Using optimized version (lupi_skd_rollout_optimized) for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Loss function defined]\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Loss Computation Functions (extended from test_lupi_skd.ipynb)\n",
    "\n",
    "def _extract_generation_logits(logits, prompt_len):\n",
    "    \"\"\"Extract logits for generation positions (aligned with tokens).\"\"\"\n",
    "    if prompt_len < 1:\n",
    "        raise ValueError('prompt_len must be >= 1')\n",
    "    return logits[:, prompt_len - 1:-1, :]\n",
    "\n",
    "def compute_lupi_kl_loss(\n",
    "    teacher_model,\n",
    "    student_model,\n",
    "    teacher_full_ids,\n",
    "    student_full_ids,\n",
    "    teacher_inputs,\n",
    "    student_inputs,\n",
    "    reduction='batchmean',\n",
    "):\n",
    "    \"\"\"Compute KL(Student || Teacher) with gradients flowing to student only.\n",
    "    \n",
    "    Returns dict with:\n",
    "        - loss: KL divergence (scalar)\n",
    "        - per_token_kl: list of per-token KL values\n",
    "        - gen_len: generation length used\n",
    "        - teacher_entropy: average teacher distribution entropy\n",
    "        - student_entropy: average student distribution entropy\n",
    "    \"\"\"\n",
    "    device = next(student_model.parameters()).device\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        teacher_outputs = teacher_model(\n",
    "            input_ids=teacher_full_ids.to(device),\n",
    "            attention_mask=torch.ones_like(teacher_full_ids).to(device),\n",
    "        )\n",
    "\n",
    "    student_outputs = student_model(\n",
    "        input_ids=student_full_ids.to(device),\n",
    "        attention_mask=torch.ones_like(student_full_ids).to(device),\n",
    "    )\n",
    "\n",
    "    teacher_logits = _extract_generation_logits(\n",
    "        teacher_outputs.logits, teacher_inputs['input_ids'].shape[1]\n",
    "    )\n",
    "    student_logits = _extract_generation_logits(\n",
    "        student_outputs.logits, student_inputs['input_ids'].shape[1]\n",
    "    )\n",
    "\n",
    "    gen_len = min(teacher_logits.shape[1], student_logits.shape[1])\n",
    "    teacher_logps = teacher_logits[:, :gen_len, :].log_softmax(dim=-1).detach()\n",
    "    student_logps = student_logits[:, :gen_len, :].log_softmax(dim=-1)\n",
    "\n",
    "    kl_loss = F.kl_div(student_logps, teacher_logps, reduction=reduction, log_target=True)\n",
    "\n",
    "    # Per-token KL for analysis\n",
    "    with torch.no_grad():\n",
    "        per_token_kl = (\n",
    "            F.kl_div(student_logps, teacher_logps, reduction='none', log_target=True)\n",
    "            .sum(dim=-1)[0]\n",
    "            .cpu()\n",
    "            .tolist()\n",
    "        )\n",
    "        \n",
    "        # Entropy metrics\n",
    "        teacher_probs = teacher_logits[:, :gen_len, :].softmax(dim=-1)\n",
    "        teacher_entropy = -(teacher_probs * teacher_probs.clamp(min=1e-10).log()).sum(dim=-1).mean().item()\n",
    "        \n",
    "        student_probs = student_logits[:, :gen_len, :].softmax(dim=-1)\n",
    "        student_entropy = -(student_probs * student_probs.clamp(min=1e-10).log()).sum(dim=-1).mean().item()\n",
    "    \n",
    "    return {\n",
    "        'loss': kl_loss,\n",
    "        'per_token_kl': per_token_kl,\n",
    "        'gen_len': gen_len,\n",
    "        'teacher_entropy': teacher_entropy,\n",
    "        'student_entropy': student_entropy,\n",
    "    }\n",
    "\n",
    "print(\"[Loss function defined]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation function defined with OPTIMIZED rollout]\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Validation Function (with detailed output) - OPTIMIZED VERSION\n",
    "\n",
    "def validate_with_details(\n",
    "    teacher_model,\n",
    "    student_model,\n",
    "    val_data,\n",
    "    config,\n",
    "    tokenizer,\n",
    "    num_generate_samples=3,\n",
    "):\n",
    "    \"\"\"Run validation with detailed logging using OPTIMIZED rollout.\n",
    "    \n",
    "    Uses lupi_skd_rollout_optimized for faster validation.\n",
    "    \"\"\"\n",
    "    student_model.eval()\n",
    "    val_losses = []\n",
    "    val_gen_lens = []\n",
    "    teacher_entropies = []\n",
    "    student_entropies = []\n",
    "    per_sample_results = []\n",
    "    \n",
    "    # Rollout statistics\n",
    "    total_acceptance_rate = 0.0\n",
    "    total_teacher_forward_calls = 0\n",
    "    \n",
    "    samples_for_logging = []\n",
    "    \n",
    "    print(f\"  Running validation on {len(val_data)} samples (using optimized rollout)...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, sample in enumerate(tqdm(val_data, desc=\"Validation\", leave=False)):\n",
    "            sample_start = time.time()\n",
    "            \n",
    "            teacher_inputs, student_inputs = build_lupi_inputs(\n",
    "                sample['query'], sample['knowledge'], tokenizer\n",
    "            )\n",
    "            \n",
    "            # Optimized Rollout (returns debug_stats)\n",
    "            teacher_full_ids, student_full_ids, gen_tokens, gen_str, rollout_stats = lupi_skd_rollout(\n",
    "                teacher_model=teacher_model,\n",
    "                student_model=student_model,\n",
    "                teacher_input_ids=teacher_inputs['input_ids'],\n",
    "                student_input_ids=student_inputs['input_ids'],\n",
    "                max_new_tokens=config.max_new_tokens,\n",
    "                top_k=config.top_k,\n",
    "                gamma=config.gamma,\n",
    "                teacher_temperature=config.teacher_temperature,\n",
    "                student_temperature=config.student_temperature,\n",
    "                teacher_top_p=config.teacher_top_p,\n",
    "                student_top_p=config.student_top_p,\n",
    "                tokenizer=tokenizer,\n",
    "                verbose=False,\n",
    "            )\n",
    "            \n",
    "            # Loss (no grad for validation)\n",
    "            loss_dict = compute_lupi_kl_loss(\n",
    "                teacher_model, student_model,\n",
    "                teacher_full_ids, student_full_ids,\n",
    "                teacher_inputs, student_inputs,\n",
    "            )\n",
    "            \n",
    "            sample_time = time.time() - sample_start\n",
    "            \n",
    "            val_losses.append(loss_dict['loss'].item())\n",
    "            val_gen_lens.append(loss_dict['gen_len'])\n",
    "            teacher_entropies.append(loss_dict['teacher_entropy'])\n",
    "            student_entropies.append(loss_dict['student_entropy'])\n",
    "            \n",
    "            # Accumulate rollout stats\n",
    "            total_acceptance_rate += rollout_stats.get('avg_acceptance_rate', 0)\n",
    "            total_teacher_forward_calls += rollout_stats.get('teacher_forward_calls', 0)\n",
    "            \n",
    "            # Per-sample result\n",
    "            per_sample_results.append({\n",
    "                'id': sample.get('id', f'val_{i}'),\n",
    "                'category': sample.get('category', 'unknown'),\n",
    "                'loss': loss_dict['loss'].item(),\n",
    "                'gen_len': loss_dict['gen_len'],\n",
    "                'teacher_entropy': loss_dict['teacher_entropy'],\n",
    "                'student_entropy': loss_dict['student_entropy'],\n",
    "                'acceptance_rate': rollout_stats.get('avg_acceptance_rate', 0),\n",
    "            })\n",
    "            \n",
    "            # Print per-sample info\n",
    "            print(f\"    Val[{i+1}/{len(val_data)}] {sample.get('id', 'N/A'):20s} | \"\n",
    "                  f\"Loss: {loss_dict['loss'].item():.4f} | \"\n",
    "                  f\"GenLen: {loss_dict['gen_len']:4d} | \"\n",
    "                  f\"AccRate: {rollout_stats.get('avg_acceptance_rate', 0):.1%} | \"\n",
    "                  f\"Time: {sample_time:.1f}s\")\n",
    "            \n",
    "            # Collect sample for logging\n",
    "            if i < num_generate_samples:\n",
    "                samples_for_logging.append({\n",
    "                    'id': sample.get('id', f'val_{i}'),\n",
    "                    'category': sample.get('category', 'unknown'),\n",
    "                    'query': sample['query'],\n",
    "                    'knowledge': sample['knowledge'][:200] + '...' if len(sample['knowledge']) > 200 else sample['knowledge'],\n",
    "                    'generated': gen_str[:500] if len(gen_str) > 500 else gen_str,\n",
    "                    'gen_len': len(gen_tokens),\n",
    "                    'loss': loss_dict['loss'].item(),\n",
    "                    'acceptance_rate': rollout_stats.get('avg_acceptance_rate', 0),\n",
    "                })\n",
    "    \n",
    "    student_model.train()\n",
    "    \n",
    "    # Compute statistics\n",
    "    avg_loss = sum(val_losses) / len(val_losses)\n",
    "    std_loss = (sum((l - avg_loss)**2 for l in val_losses) / len(val_losses)) ** 0.5\n",
    "    avg_acceptance_rate = total_acceptance_rate / len(val_data)\n",
    "    \n",
    "    # Category breakdown\n",
    "    category_stats = {}\n",
    "    for r in per_sample_results:\n",
    "        cat = r['category']\n",
    "        if cat not in category_stats:\n",
    "            category_stats[cat] = {'losses': [], 'gen_lens': [], 'acceptance_rates': []}\n",
    "        category_stats[cat]['losses'].append(r['loss'])\n",
    "        category_stats[cat]['gen_lens'].append(r['gen_len'])\n",
    "        category_stats[cat]['acceptance_rates'].append(r['acceptance_rate'])\n",
    "    \n",
    "    print(f\"\\n  [Validation Statistics]\")\n",
    "    print(f\"    Overall: Loss = {avg_loss:.4f} Â± {std_loss:.4f}\")\n",
    "    print(f\"    Avg Acceptance Rate: {avg_acceptance_rate:.1%}\")\n",
    "    print(f\"    Total Teacher Forward Calls: {total_teacher_forward_calls}\")\n",
    "    print(f\"    Per-category breakdown:\")\n",
    "    for cat, stats in sorted(category_stats.items()):\n",
    "        cat_avg = sum(stats['losses']) / len(stats['losses'])\n",
    "        cat_gen = sum(stats['gen_lens']) / len(stats['gen_lens'])\n",
    "        cat_acc = sum(stats['acceptance_rates']) / len(stats['acceptance_rates'])\n",
    "        print(f\"      {cat:15s}: Loss = {cat_avg:.4f}, Avg GenLen = {cat_gen:.1f}, AccRate = {cat_acc:.1%} (n={len(stats['losses'])})\")\n",
    "    \n",
    "    return {\n",
    "        'val_loss': avg_loss,\n",
    "        'val_loss_std': std_loss,\n",
    "        'val_gen_len_avg': sum(val_gen_lens) / len(val_gen_lens),\n",
    "        'val_teacher_entropy': sum(teacher_entropies) / len(teacher_entropies),\n",
    "        'val_student_entropy': sum(student_entropies) / len(student_entropies),\n",
    "        'val_acceptance_rate': avg_acceptance_rate,\n",
    "        'samples': samples_for_logging,\n",
    "        'per_sample_results': per_sample_results,\n",
    "        'category_stats': category_stats,\n",
    "    }\n",
    "\n",
    "print(\"[Validation function defined with OPTIMIZED rollout]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33meoeldroal\u001b[0m (\u001b[33meoeldroal-sogang-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/eoeldroal/WorkPlace/SKD_RAG_K/notebooks/wandb/run-20251205_173730-j1igjb6s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/eoeldroal-sogang-university/LUPI-SKD-Korean-Culture/runs/j1igjb6s' target=\"_blank\">lupi-skd-k5-lr2e-06-20251205_1737</a></strong> to <a href='https://wandb.ai/eoeldroal-sogang-university/LUPI-SKD-Korean-Culture' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/eoeldroal-sogang-university/LUPI-SKD-Korean-Culture' target=\"_blank\">https://wandb.ai/eoeldroal-sogang-university/LUPI-SKD-Korean-Culture</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/eoeldroal-sogang-university/LUPI-SKD-Korean-Culture/runs/j1igjb6s' target=\"_blank\">https://wandb.ai/eoeldroal-sogang-university/LUPI-SKD-Korean-Culture/runs/j1igjb6s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WandB Initialized]\n",
      "  Project: LUPI-SKD-Korean-Culture\n",
      "  Run: lupi-skd-k5-lr2e-06-20251205_1737\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: WandB Initialization\n",
    "\n",
    "def init_wandb(config: LUPISKDConfig):\n",
    "    \"\"\"Initialize WandB with comprehensive config.\"\"\"\n",
    "    run_name = config.wandb_run_name or f\"lupi-skd-k{config.top_k}-lr{config.learning_rate}-{datetime.now().strftime('%Y%m%d_%H%M')}\"\n",
    "    \n",
    "    wandb.init(\n",
    "        project=config.wandb_project,\n",
    "        name=run_name,\n",
    "        config={\n",
    "            # Model\n",
    "            \"teacher_model\": config.teacher_model_name,\n",
    "            \"student_model\": config.student_model_name,\n",
    "            \n",
    "            # SKD\n",
    "            \"top_k\": config.top_k,\n",
    "            \"gamma\": config.gamma,\n",
    "            \"teacher_temperature\": config.teacher_temperature,\n",
    "            \"student_temperature\": config.student_temperature,\n",
    "            \"teacher_top_p\": config.teacher_top_p,\n",
    "            \"student_top_p\": config.student_top_p,\n",
    "            \n",
    "            # Training\n",
    "            \"epochs\": config.epochs,\n",
    "            \"learning_rate\": config.learning_rate,\n",
    "            \"gradient_accumulation_steps\": config.gradient_accumulation_steps,\n",
    "            \"max_grad_norm\": config.max_grad_norm,\n",
    "            \"max_new_tokens\": config.max_new_tokens,\n",
    "            \"warmup_ratio\": config.warmup_ratio,\n",
    "            \n",
    "            # Data\n",
    "            \"train_samples\": int(200 * config.train_val_split),\n",
    "            \"val_samples\": int(200 * (1 - config.train_val_split)),\n",
    "            \"data_source\": \"korean_culture_train_200.json\",\n",
    "            \n",
    "            # Misc\n",
    "            \"seed\": config.seed,\n",
    "        },\n",
    "        tags=[\"LUPI-SKD\", \"Korean-Culture\", \"Qwen3\"],\n",
    "    )\n",
    "    \n",
    "    return run_name\n",
    "\n",
    "run_name = init_wandb(config)\n",
    "print(f\"[WandB Initialized]\")\n",
    "print(f\"  Project: {config.wandb_project}\")\n",
    "print(f\"  Run: {run_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training Setup]\n",
      "  Train samples: 180\n",
      "  Grad accum: 4\n",
      "  Steps per epoch: 45\n",
      "  Total training steps: 135\n",
      "  Warmup steps: 13\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Optimizer and Scheduler Setup\n",
    "\n",
    "def setup_training(student_model, config, num_training_steps):\n",
    "    \"\"\"Setup optimizer and scheduler.\"\"\"\n",
    "    optimizer = AdamW(\n",
    "        student_model.parameters(),\n",
    "        lr=config.learning_rate,\n",
    "        weight_decay=0.01,\n",
    "    )\n",
    "    \n",
    "    num_warmup_steps = int(num_training_steps * config.warmup_ratio)\n",
    "    \n",
    "    scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_training_steps,\n",
    "    )\n",
    "    \n",
    "    return optimizer, scheduler\n",
    "\n",
    "# Calculate total steps\n",
    "steps_per_epoch = len(train_data) // config.gradient_accumulation_steps\n",
    "total_steps = steps_per_epoch * config.epochs\n",
    "\n",
    "print(f\"[Training Setup]\")\n",
    "print(f\"  Train samples: {len(train_data)}\")\n",
    "print(f\"  Grad accum: {config.gradient_accumulation_steps}\")\n",
    "print(f\"  Steps per epoch: {steps_per_epoch}\")\n",
    "print(f\"  Total training steps: {total_steps}\")\n",
    "print(f\"  Warmup steps: {int(total_steps * config.warmup_ratio)}\")\n",
    "\n",
    "optimizer, scheduler = setup_training(student_model, config, total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Checkpoint directory: /home/eoeldroal/WorkPlace/SKD_RAG_K/checkpoints/lupi_skd]\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Checkpoint Functions\n",
    "\n",
    "def save_checkpoint(student_model, tokenizer, config, epoch, step, val_loss):\n",
    "    \"\"\"Save checkpoint with metadata.\"\"\"\n",
    "    ckpt_dir = Path(config.checkpoint_dir)\n",
    "    ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    save_path = ckpt_dir / f\"epoch{epoch}_step{step}_valloss{val_loss:.4f}\"\n",
    "    save_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    student_model.save_pretrained(save_path)\n",
    "    tokenizer.save_pretrained(save_path)\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        \"epoch\": epoch,\n",
    "        \"step\": step,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"config\": {\n",
    "            \"top_k\": config.top_k,\n",
    "            \"gamma\": config.gamma,\n",
    "            \"learning_rate\": config.learning_rate,\n",
    "            \"teacher_model\": config.teacher_model_name,\n",
    "            \"student_model\": config.student_model_name,\n",
    "        }\n",
    "    }\n",
    "    with open(save_path / \"metadata.json\", 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"  [Checkpoint saved: {save_path.name}]\")\n",
    "    return str(save_path)\n",
    "\n",
    "print(f\"[Checkpoint directory: {config.checkpoint_dir}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training function defined with OPTIMIZED rollout]\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Main Training Loop (with OPTIMIZED rollout)\n",
    "\n",
    "def train_lupi_skd(\n",
    "    teacher_model,\n",
    "    student_model,\n",
    "    train_data,\n",
    "    val_data,\n",
    "    config,\n",
    "    tokenizer,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "):\n",
    "    \"\"\"Main LUPI-SKD training loop with OPTIMIZED rollout and detailed logging.\"\"\"\n",
    "    student_model.train()\n",
    "    \n",
    "    global_step = 0\n",
    "    accumulated_loss = 0.0\n",
    "    accumulated_samples = 0\n",
    "    \n",
    "    # Metrics accumulators for logging\n",
    "    step_gen_lens = []\n",
    "    step_teacher_entropies = []\n",
    "    step_student_entropies = []\n",
    "    step_acceptance_rates = []\n",
    "    step_teacher_forward_calls = []\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    training_log = []\n",
    "    \n",
    "    # Initial validation before training\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"[INITIAL VALIDATION] Before training starts\")\n",
    "    print(\"=\"*70)\n",
    "    val_results = validate_with_details(\n",
    "        teacher_model, student_model, val_data,\n",
    "        config, tokenizer, num_generate_samples=2\n",
    "    )\n",
    "    print(f\"  Initial Val Loss: {val_results['val_loss']:.4f}\")\n",
    "    print(f\"  Initial Val Gen Len: {val_results['val_gen_len_avg']:.1f}\")\n",
    "    print(f\"  Initial Val Acceptance Rate: {val_results.get('val_acceptance_rate', 0):.1%}\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    for epoch in range(1, config.epochs + 1):\n",
    "        print(f\"\\n{'#'*70}\")\n",
    "        print(f\"#  EPOCH {epoch}/{config.epochs}\")\n",
    "        print(f\"{'#'*70}\")\n",
    "        \n",
    "        # Shuffle training data each epoch\n",
    "        random.shuffle(train_data)\n",
    "        \n",
    "        epoch_start_time = time.time()\n",
    "        epoch_losses = []\n",
    "        epoch_gen_lens = []\n",
    "        epoch_acceptance_rates = []\n",
    "        \n",
    "        pbar = tqdm(enumerate(train_data), total=len(train_data), desc=f\"Epoch {epoch}\")\n",
    "        \n",
    "        for sample_idx, sample in pbar:\n",
    "            sample_start_time = time.time()\n",
    "            \n",
    "            # Build LUPI inputs\n",
    "            teacher_inputs, student_inputs = build_lupi_inputs(\n",
    "                sample['query'], sample['knowledge'], tokenizer\n",
    "            )\n",
    "            \n",
    "            # LUPI-SKD Rollout using OPTIMIZED version (no grad for rollout itself)\n",
    "            with torch.no_grad():\n",
    "                teacher_full_ids, student_full_ids, gen_tokens, gen_str, rollout_stats = lupi_skd_rollout(\n",
    "                    teacher_model=teacher_model,\n",
    "                    student_model=student_model,\n",
    "                    teacher_input_ids=teacher_inputs['input_ids'],\n",
    "                    student_input_ids=student_inputs['input_ids'],\n",
    "                    max_new_tokens=config.max_new_tokens,\n",
    "                    top_k=config.top_k,\n",
    "                    gamma=config.gamma,\n",
    "                    teacher_temperature=config.teacher_temperature,\n",
    "                    student_temperature=config.student_temperature,\n",
    "                    teacher_top_p=config.teacher_top_p,\n",
    "                    student_top_p=config.student_top_p,\n",
    "                    tokenizer=tokenizer,\n",
    "                    verbose=False,\n",
    "                )\n",
    "            \n",
    "            # Compute KL Loss (with grad)\n",
    "            loss_dict = compute_lupi_kl_loss(\n",
    "                teacher_model, student_model,\n",
    "                teacher_full_ids, student_full_ids,\n",
    "                teacher_inputs, student_inputs,\n",
    "            )\n",
    "            \n",
    "            loss = loss_dict['loss'] / config.gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "            \n",
    "            sample_time = time.time() - sample_start_time\n",
    "            \n",
    "            # Accumulate metrics\n",
    "            accumulated_loss += loss_dict['loss'].item()\n",
    "            accumulated_samples += 1\n",
    "            epoch_losses.append(loss_dict['loss'].item())\n",
    "            epoch_gen_lens.append(loss_dict['gen_len'])\n",
    "            epoch_acceptance_rates.append(rollout_stats.get('avg_acceptance_rate', 0))\n",
    "            \n",
    "            step_gen_lens.append(loss_dict['gen_len'])\n",
    "            step_teacher_entropies.append(loss_dict['teacher_entropy'])\n",
    "            step_student_entropies.append(loss_dict['student_entropy'])\n",
    "            step_acceptance_rates.append(rollout_stats.get('avg_acceptance_rate', 0))\n",
    "            step_teacher_forward_calls.append(rollout_stats.get('teacher_forward_calls', 0))\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'loss': f\"{loss_dict['loss'].item():.4f}\",\n",
    "                'gen_len': loss_dict['gen_len'],\n",
    "                'acc_rate': f\"{rollout_stats.get('avg_acceptance_rate', 0):.0%}\",\n",
    "                'time': f\"{sample_time:.1f}s\"\n",
    "            })\n",
    "            \n",
    "            # Detailed sample logging (every N samples)\n",
    "            if (sample_idx + 1) % 10 == 0 or sample_idx == 0:\n",
    "                print(f\"\\n  [Sample {sample_idx+1}/{len(train_data)}] ID: {sample.get('id', 'N/A')}\")\n",
    "                print(f\"    Category: {sample.get('category', 'N/A')}\")\n",
    "                print(f\"    Query: {sample['query'][:60]}...\")\n",
    "                print(f\"    Loss: {loss_dict['loss'].item():.4f} | Gen Len: {loss_dict['gen_len']}\")\n",
    "                print(f\"    Acceptance Rate: {rollout_stats.get('avg_acceptance_rate', 0):.1%}\")\n",
    "                print(f\"    Teacher Forward Calls: {rollout_stats.get('teacher_forward_calls', 0)}\")\n",
    "                print(f\"    Entropy Gap (S-T): {loss_dict['student_entropy'] - loss_dict['teacher_entropy']:.3f}\")\n",
    "                print(f\"    Time: {sample_time:.2f}s\")\n",
    "                print(f\"    Generated (first 100 chars): {gen_str[:100]}...\")\n",
    "            \n",
    "            # Gradient accumulation step\n",
    "            if (sample_idx + 1) % config.gradient_accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(student_model.parameters(), config.max_grad_norm)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                global_step += 1\n",
    "                avg_loss = accumulated_loss / accumulated_samples\n",
    "                avg_gen_len = sum(step_gen_lens) / len(step_gen_lens)\n",
    "                avg_teacher_ent = sum(step_teacher_entropies) / len(step_teacher_entropies)\n",
    "                avg_student_ent = sum(step_student_entropies) / len(step_student_entropies)\n",
    "                avg_acceptance_rate = sum(step_acceptance_rates) / len(step_acceptance_rates)\n",
    "                avg_teacher_calls = sum(step_teacher_forward_calls) / len(step_teacher_forward_calls)\n",
    "                \n",
    "                # Print step summary\n",
    "                print(f\"\\n  >>> [Step {global_step}] Optimizer Step <<<\")\n",
    "                print(f\"      Avg Loss (accum): {avg_loss:.4f}\")\n",
    "                print(f\"      Avg Gen Len: {avg_gen_len:.1f}\")\n",
    "                print(f\"      Avg Acceptance Rate: {avg_acceptance_rate:.1%}\")\n",
    "                print(f\"      Avg Teacher Forward Calls: {avg_teacher_calls:.1f}\")\n",
    "                print(f\"      Avg Entropy Gap: {avg_student_ent - avg_teacher_ent:.3f}\")\n",
    "                print(f\"      Learning Rate: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "                \n",
    "                # Log to training_log\n",
    "                log_dict = {\n",
    "                    \"train/loss\": avg_loss,\n",
    "                    \"train/learning_rate\": scheduler.get_last_lr()[0],\n",
    "                    \"train/epoch\": epoch,\n",
    "                    \"train/global_step\": global_step,\n",
    "                    \"train/gen_len_avg\": avg_gen_len,\n",
    "                    \"train/teacher_entropy\": avg_teacher_ent,\n",
    "                    \"train/student_entropy\": avg_student_ent,\n",
    "                    \"train/entropy_gap\": avg_student_ent - avg_teacher_ent,\n",
    "                    \"train/acceptance_rate\": avg_acceptance_rate,\n",
    "                    \"train/teacher_forward_calls\": avg_teacher_calls,\n",
    "                }\n",
    "                training_log.append({'step': global_step, 'epoch': epoch, **log_dict})\n",
    "                \n",
    "                # Reset accumulators\n",
    "                accumulated_loss = 0.0\n",
    "                accumulated_samples = 0\n",
    "                step_gen_lens = []\n",
    "                step_teacher_entropies = []\n",
    "                step_student_entropies = []\n",
    "                step_acceptance_rates = []\n",
    "                step_teacher_forward_calls = []\n",
    "        \n",
    "        # ========== END OF EPOCH ==========\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        epoch_avg_loss = sum(epoch_losses) / len(epoch_losses) if epoch_losses else 0\n",
    "        epoch_avg_gen_len = sum(epoch_gen_lens) / len(epoch_gen_lens) if epoch_gen_lens else 0\n",
    "        epoch_avg_acceptance = sum(epoch_acceptance_rates) / len(epoch_acceptance_rates) if epoch_acceptance_rates else 0\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"[EPOCH {epoch} SUMMARY]\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"  Time: {epoch_time/60:.2f} min ({epoch_time:.0f} sec)\")\n",
    "        print(f\"  Samples processed: {len(train_data)}\")\n",
    "        print(f\"  Avg Loss: {epoch_avg_loss:.4f}\")\n",
    "        print(f\"  Avg Gen Len: {epoch_avg_gen_len:.1f}\")\n",
    "        print(f\"  Avg Acceptance Rate: {epoch_avg_acceptance:.1%}\")\n",
    "        print(f\"  Global Step: {global_step}\")\n",
    "        \n",
    "        # ========== EPOCH VALIDATION ==========\n",
    "        print(f\"\\n{'-'*70}\")\n",
    "        print(f\"[EPOCH {epoch} VALIDATION]\")\n",
    "        print(f\"{'-'*70}\")\n",
    "        \n",
    "        val_results = validate_with_details(\n",
    "            teacher_model, student_model, val_data,\n",
    "            config, tokenizer, num_generate_samples=3\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n  Val Loss: {val_results['val_loss']:.4f}\")\n",
    "        print(f\"  Val Gen Len Avg: {val_results['val_gen_len_avg']:.1f}\")\n",
    "        print(f\"  Val Acceptance Rate: {val_results.get('val_acceptance_rate', 0):.1%}\")\n",
    "        print(f\"  Val Teacher Entropy: {val_results['val_teacher_entropy']:.3f}\")\n",
    "        print(f\"  Val Student Entropy: {val_results['val_student_entropy']:.3f}\")\n",
    "        \n",
    "        # Show sample generations\n",
    "        print(f\"\\n  [Sample Generations from Validation]\")\n",
    "        for i, s in enumerate(val_results['samples']):\n",
    "            print(f\"\\n  --- Val Sample {i+1} ({s['category']}) ---\")\n",
    "            print(f\"  Query: {s['query'][:80]}...\")\n",
    "            print(f\"  Gen Len: {s['gen_len']} tokens, AccRate: {s.get('acceptance_rate', 0):.1%}\")\n",
    "            print(f\"  Generated:\\n    {s['generated'][:300]}...\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_results['val_loss'] < best_val_loss:\n",
    "            best_val_loss = val_results['val_loss']\n",
    "            print(f\"\\n  *** NEW BEST VAL LOSS: {best_val_loss:.4f} ***\")\n",
    "            save_checkpoint(student_model, tokenizer, config, epoch, global_step, best_val_loss)\n",
    "        \n",
    "        # Save checkpoint per epoch\n",
    "        if config.save_every_epoch:\n",
    "            save_checkpoint(student_model, tokenizer, config, epoch, global_step, epoch_avg_loss)\n",
    "        \n",
    "        print(f\"\\n  Best Val Loss so far: {best_val_loss:.4f}\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "        \n",
    "        student_model.train()\n",
    "    \n",
    "    return best_val_loss, training_log\n",
    "\n",
    "print(\"[Training function defined with OPTIMIZED rollout]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "   LUPI-SKD TRAINING SESSION\n",
      "======================================================================\n",
      "\n",
      "[DATA]\n",
      "  Train samples: 180\n",
      "  Val samples: 20\n",
      "\n",
      "[MODEL]\n",
      "  Teacher: Qwen/Qwen3-32B\n",
      "  Student: Qwen/Qwen3-4B-Thinking-2507\n",
      "\n",
      "[SKD HYPERPARAMETERS]\n",
      "  Top-K: 5\n",
      "  Gamma (block size): 30\n",
      "  Teacher Temperature: 0.2\n",
      "  Student Temperature: 0.5\n",
      "\n",
      "[TRAINING]\n",
      "  Epochs: 3\n",
      "  Learning Rate: 2e-06\n",
      "  Gradient Accumulation: 4\n",
      "  Max New Tokens: 8192\n",
      "  Steps per Epoch: 45\n",
      "  Total Steps: 135\n",
      "  Warmup Steps: 13\n",
      "\n",
      "[CHECKPOINT]\n",
      "  Save directory: /home/eoeldroal/WorkPlace/SKD_RAG_K/checkpoints/lupi_skd\n",
      "  Save every epoch: True\n",
      "\n",
      "======================================================================\n",
      "   STARTING TRAINING...\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "[INITIAL VALIDATION] Before training starts\n",
      "======================================================================\n",
      "  Running validation on 20 samples (using optimized rollout)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d8a20e658254c9fb1e194f0f10dcbba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m70\u001b[39m + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     38\u001b[39m training_start_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m best_val_loss, training_log = \u001b[43mtrain_lupi_skd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43mteacher_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mteacher_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstudent_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstudent_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m total_training_time = time.time() - training_start_time\n\u001b[32m     53\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m70\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mtrain_lupi_skd\u001b[39m\u001b[34m(teacher_model, student_model, train_data, val_data, config, tokenizer, optimizer, scheduler)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m[INITIAL VALIDATION] Before training starts\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m70\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m val_results = \u001b[43mvalidate_with_details\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mteacher_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstudent_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_generate_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\n\u001b[32m     37\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Initial Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_results[\u001b[33m'\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Initial Val Gen Len: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_results[\u001b[33m'\u001b[39m\u001b[33mval_gen_len_avg\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mvalidate_with_details\u001b[39m\u001b[34m(teacher_model, student_model, val_data, config, tokenizer, num_generate_samples)\u001b[39m\n\u001b[32m     34\u001b[39m teacher_inputs, student_inputs = build_lupi_inputs(\n\u001b[32m     35\u001b[39m     sample[\u001b[33m'\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m'\u001b[39m], sample[\u001b[33m'\u001b[39m\u001b[33mknowledge\u001b[39m\u001b[33m'\u001b[39m], tokenizer\n\u001b[32m     36\u001b[39m )\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Optimized Rollout (returns debug_stats)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m teacher_full_ids, student_full_ids, gen_tokens, gen_str, rollout_stats = \u001b[43mlupi_skd_rollout\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mteacher_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mteacher_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstudent_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstudent_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43mteacher_input_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mteacher_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minput_ids\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstudent_input_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstudent_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minput_ids\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mteacher_temperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mteacher_temperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstudent_temperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstudent_temperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mteacher_top_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mteacher_top_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstudent_top_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstudent_top_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# Loss (no grad for validation)\u001b[39;00m\n\u001b[32m     56\u001b[39m loss_dict = compute_lupi_kl_loss(\n\u001b[32m     57\u001b[39m     teacher_model, student_model,\n\u001b[32m     58\u001b[39m     teacher_full_ids, student_full_ids,\n\u001b[32m     59\u001b[39m     teacher_inputs, student_inputs,\n\u001b[32m     60\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/SKD/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/WorkPlace/SKD_RAG_K/train/lupi_rollout_optimized.py:361\u001b[39m, in \u001b[36mlupi_skd_rollout_optimized\u001b[39m\u001b[34m(teacher_model, student_model, teacher_input_ids, student_input_ids, max_new_tokens, top_k, gamma, teacher_temperature, student_temperature, teacher_top_p, student_top_p, tokenizer, end_of_string_ls, verbose, print_every_blocks)\u001b[39m\n\u001b[32m    358\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m block_student_past \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    359\u001b[39m     student_kwargs[\u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m] = block_student_past\n\u001b[32m--> \u001b[39m\u001b[32m361\u001b[39m student_out = \u001b[43mstudent_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mstudent_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    362\u001b[39m block_student_past = student_out.past_key_values\n\u001b[32m    363\u001b[39m student_logits = student_out.logits[:, -\u001b[32m1\u001b[39m, :] / \u001b[38;5;28mmax\u001b[39m(student_temperature, \u001b[32m1e-6\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/SKD/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/SKD/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/SKD/lib/python3.11/site-packages/accelerate/hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/SKD/lib/python3.11/site-packages/transformers/utils/generic.py:918\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    917\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    920\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/SKD/lib/python3.11/site-packages/transformers/models/qwen3/modeling_qwen3.py:480\u001b[39m, in \u001b[36mQwen3ForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    443\u001b[39m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[32m    444\u001b[39m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[32m    445\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m    456\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m    457\u001b[39m ) -> CausalLMOutputWithPast:\n\u001b[32m    458\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    459\u001b[39m \u001b[33;03m    labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[32m    460\u001b[39m \u001b[33;03m        Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    478\u001b[39m \u001b[33;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[39;00m\n\u001b[32m    479\u001b[39m \u001b[33;03m    ```\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m     outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    491\u001b[39m     hidden_states = outputs.last_hidden_state\n\u001b[32m    492\u001b[39m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/SKD/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/SKD/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/SKD/lib/python3.11/site-packages/transformers/utils/generic.py:1064\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1061\u001b[39m                 monkey_patched_layers.append((module, original_forward))\n\u001b[32m   1063\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1064\u001b[39m     outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1065\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1066\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1067\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1068\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[32m   1069\u001b[39m     kwargs_without_recordable = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/SKD/lib/python3.11/site-packages/transformers/models/qwen3/modeling_qwen3.py:410\u001b[39m, in \u001b[36mQwen3Model.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[39m\n\u001b[32m    407\u001b[39m position_embeddings = \u001b[38;5;28mself\u001b[39m.rotary_emb(hidden_states, position_ids)\n\u001b[32m    409\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers[: \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers]:\n\u001b[32m--> \u001b[39m\u001b[32m410\u001b[39m     hidden_states = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattention_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    418\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    419\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.norm(hidden_states)\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[32m    423\u001b[39m     last_hidden_state=hidden_states,\n\u001b[32m    424\u001b[39m     past_key_values=past_key_values \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    425\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/SKD/lib/python3.11/site-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/SKD/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/SKD/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/SKD/lib/python3.11/site-packages/accelerate/hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/SKD/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/SKD/lib/python3.11/site-packages/transformers/models/qwen3/modeling_qwen3.py:274\u001b[39m, in \u001b[36mQwen3DecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    272\u001b[39m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n\u001b[32m    273\u001b[39m residual = hidden_states\n\u001b[32m--> \u001b[39m\u001b[32m274\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpost_attention_layernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    275\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.mlp(hidden_states)\n\u001b[32m    276\u001b[39m hidden_states = residual + hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/SKD/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/SKD/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/SKD/lib/python3.11/site-packages/accelerate/hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/SKD/lib/python3.11/site-packages/transformers/models/qwen3/modeling_qwen3.py:61\u001b[39m, in \u001b[36mQwen3RMSNorm.forward\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch.Tensor) -> torch.Tensor:\n\u001b[32m     60\u001b[39m     input_dtype = hidden_states.dtype\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     hidden_states = \u001b[43mhidden_states\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m     variance = hidden_states.pow(\u001b[32m2\u001b[39m).mean(-\u001b[32m1\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     63\u001b[39m     hidden_states = hidden_states * torch.rsqrt(variance + \u001b[38;5;28mself\u001b[39m.variance_epsilon)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._post_run_cell_hook of <wandb.sdk.wandb_init._WandbInit object at 0x79d5189b0190>> (for post_run_cell), with arguments args (<ExecutionResult object at 79d51be62a10, execution_count=13 error_before_exec=None error_in_exec= info=<ExecutionInfo object at 79d51be62510, raw_cell=\"# Cell 12: Execute Training\n",
      "\n",
      "print(\"\\n\" + \"=\"*70)\n",
      "..\" transformed_cell=\"# Cell 12: Execute Training\n",
      "\n",
      "print(\"\\n\" + \"=\"*70)\n",
      "..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2249534453227d/home/eoeldroal/WorkPlace/SKD_RAG_K/notebooks/train_lupi_skd_full.ipynb#X55sdnNjb2RlLXJlbW90ZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "ConnectionResetError",
     "evalue": "Connection lost",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionResetError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/SKD/lib/python3.11/site-packages/wandb/sdk/wandb_init.py:604\u001b[39m, in \u001b[36m_WandbInit._post_run_cell_hook\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    601\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    603\u001b[39m \u001b[38;5;28mself\u001b[39m._logger.info(\u001b[33m\"\u001b[39m\u001b[33mresuming backend\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43minterface\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpublish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/SKD/lib/python3.11/site-packages/wandb/sdk/interface/interface.py:811\u001b[39m, in \u001b[36mInterfaceBase.publish_resume\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    809\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpublish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    810\u001b[39m     resume = pb.ResumeRequest()\n\u001b[32m--> \u001b[39m\u001b[32m811\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_publish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/SKD/lib/python3.11/site-packages/wandb/sdk/interface/interface_shared.py:334\u001b[39m, in \u001b[36mInterfaceShared._publish_resume\u001b[39m\u001b[34m(self, resume)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_publish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m, resume: pb.ResumeRequest) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    333\u001b[39m     rec = \u001b[38;5;28mself\u001b[39m._make_request(resume=resume)\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/SKD/lib/python3.11/site-packages/wandb/sdk/interface/interface_sock.py:46\u001b[39m, in \u001b[36mInterfaceSock._publish\u001b[39m\u001b[34m(self, record, nowait)\u001b[39m\n\u001b[32m     44\u001b[39m     \u001b[38;5;28mself\u001b[39m._asyncer.run_soon(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m._client.publish(request))\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_asyncer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpublish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/SKD/lib/python3.11/site-packages/wandb/sdk/lib/asyncio_manager.py:136\u001b[39m, in \u001b[36mAsyncioManager.run\u001b[39m\u001b[34m(self, fn)\u001b[39m\n\u001b[32m    133\u001b[39m future = \u001b[38;5;28mself\u001b[39m._schedule(fn, daemon=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m concurrent.futures.CancelledError:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RunCancelledError \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/SKD/lib/python3.11/concurrent/futures/_base.py:456\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/SKD/lib/python3.11/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/SKD/lib/python3.11/site-packages/wandb/sdk/lib/asyncio_manager.py:219\u001b[39m, in \u001b[36mAsyncioManager._wrap\u001b[39m\u001b[34m(self, fn, daemon, name)\u001b[39m\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mand\u001b[39;00m (task := asyncio.current_task()):\n\u001b[32m    217\u001b[39m         task.set_name(name)\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fn()\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    221\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m daemon:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/SKD/lib/python3.11/site-packages/wandb/sdk/lib/service/service_client.py:38\u001b[39m, in \u001b[36mServiceClient.publish\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpublish\u001b[39m(\u001b[38;5;28mself\u001b[39m, request: spb.ServerRequest) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     37\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Send a request without waiting for a response.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_server_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/SKD/lib/python3.11/site-packages/wandb/sdk/lib/service/service_client.py:64\u001b[39m, in \u001b[36mServiceClient._send_server_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     61\u001b[39m data = request.SerializeToString()\n\u001b[32m     62\u001b[39m \u001b[38;5;28mself\u001b[39m._writer.write(data)\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._writer.drain()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/SKD/lib/python3.11/asyncio/streams.py:392\u001b[39m, in \u001b[36mStreamWriter.drain\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    381\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._transport.is_closing():\n\u001b[32m    382\u001b[39m     \u001b[38;5;66;03m# Wait for protocol.connection_lost() call\u001b[39;00m\n\u001b[32m    383\u001b[39m     \u001b[38;5;66;03m# Raise connection closing error if any,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    389\u001b[39m     \u001b[38;5;66;03m# in a loop would never call connection_lost(), so it\u001b[39;00m\n\u001b[32m    390\u001b[39m     \u001b[38;5;66;03m# would not see an error when the socket is closed.\u001b[39;00m\n\u001b[32m    391\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m sleep(\u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m392\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol._drain_helper()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/SKD/lib/python3.11/asyncio/streams.py:166\u001b[39m, in \u001b[36mFlowControlMixin._drain_helper\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_drain_helper\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    165\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection_lost:\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionResetError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mConnection lost\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._paused:\n\u001b[32m    168\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[31mConnectionResetError\u001b[39m: Connection lost"
     ]
    }
   ],
   "source": [
    "# Cell 12: Execute Training\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"   LUPI-SKD TRAINING SESSION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n[DATA]\")\n",
    "print(f\"  Train samples: {len(train_data)}\")\n",
    "print(f\"  Val samples: {len(val_data)}\")\n",
    "\n",
    "print(\"\\n[MODEL]\")\n",
    "print(f\"  Teacher: {config.teacher_model_name}\")\n",
    "print(f\"  Student: {config.student_model_name}\")\n",
    "\n",
    "print(\"\\n[SKD HYPERPARAMETERS]\")\n",
    "print(f\"  Top-K: {config.top_k}\")\n",
    "print(f\"  Gamma (block size): {config.gamma}\")\n",
    "print(f\"  Teacher Temperature: {config.teacher_temperature}\")\n",
    "print(f\"  Student Temperature: {config.student_temperature}\")\n",
    "\n",
    "print(\"\\n[TRAINING]\")\n",
    "print(f\"  Epochs: {config.epochs}\")\n",
    "print(f\"  Learning Rate: {config.learning_rate}\")\n",
    "print(f\"  Gradient Accumulation: {config.gradient_accumulation_steps}\")\n",
    "print(f\"  Max New Tokens: {config.max_new_tokens}\")\n",
    "print(f\"  Steps per Epoch: {steps_per_epoch}\")\n",
    "print(f\"  Total Steps: {total_steps}\")\n",
    "print(f\"  Warmup Steps: {int(total_steps * config.warmup_ratio)}\")\n",
    "\n",
    "print(\"\\n[CHECKPOINT]\")\n",
    "print(f\"  Save directory: {config.checkpoint_dir}\")\n",
    "print(f\"  Save every epoch: {config.save_every_epoch}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"   STARTING TRAINING...\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "training_start_time = time.time()\n",
    "\n",
    "best_val_loss, training_log = train_lupi_skd(\n",
    "    teacher_model=teacher_model,\n",
    "    student_model=student_model,\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    config=config,\n",
    "    tokenizer=tokenizer,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    ")\n",
    "\n",
    "total_training_time = time.time() - training_start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"   TRAINING COMPLETED!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n  Total Time: {total_training_time/60:.2f} min ({total_training_time/3600:.2f} hours)\")\n",
    "print(f\"  Best Val Loss: {best_val_loss:.4f}\")\n",
    "print(f\"  Total Steps: {len(training_log)}\")\n",
    "\n",
    "# Summary of training progression\n",
    "if training_log:\n",
    "    first_loss = training_log[0]['train/loss']\n",
    "    last_loss = training_log[-1]['train/loss']\n",
    "    print(f\"\\n  Loss Progression:\")\n",
    "    print(f\"    First step: {first_loss:.4f}\")\n",
    "    print(f\"    Last step:  {last_loss:.4f}\")\n",
    "    print(f\"    Change:     {last_loss - first_loss:.4f} ({(last_loss/first_loss - 1)*100:+.1f}%)\")\n",
    "\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Final Evaluation and Sample Generation\n",
    "\n",
    "def final_evaluation(student_model, val_data, config, tokenizer):\n",
    "    \"\"\"Generate outputs for all validation samples.\"\"\"\n",
    "    student_model.eval()\n",
    "    device = next(student_model.parameters()).device\n",
    "    results = []\n",
    "    \n",
    "    print(\"Generating final evaluation samples...\")\n",
    "    for sample in tqdm(val_data, desc=\"Final Evaluation\"):\n",
    "        student_inputs = tokenizer.apply_chat_template(\n",
    "            [{\"role\": \"user\", \"content\": sample['query']}],\n",
    "            return_dict=True,\n",
    "            tokenize=True,\n",
    "            return_tensors=\"pt\",\n",
    "            add_generation_prompt=True,\n",
    "            enable_thinking=True,\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            generated = student_model.generate(\n",
    "                input_ids=student_inputs['input_ids'].to(device),\n",
    "                max_new_tokens=config.max_new_tokens,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "            )\n",
    "        \n",
    "        gen_text = tokenizer.decode(\n",
    "            generated[0][student_inputs['input_ids'].shape[1]:], \n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            'id': sample.get('id', 'unknown'),\n",
    "            'category': sample.get('category', 'unknown'),\n",
    "            'query': sample['query'],\n",
    "            'knowledge': sample['knowledge'],\n",
    "            'generated': gen_text,\n",
    "        })\n",
    "    \n",
    "    # Save to file\n",
    "    output_path = Path(config.checkpoint_dir) / \"final_val_generations.json\"\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # Save training log\n",
    "    log_path = Path(config.checkpoint_dir) / \"training_log.json\"\n",
    "    with open(log_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(training_log, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n[Final Evaluation Complete]\")\n",
    "    print(f\"  Saved {len(results)} generations to {output_path}\")\n",
    "    print(f\"  Saved training log to {log_path}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "final_results = final_evaluation(student_model, val_data, config, tokenizer)\n",
    "\n",
    "# Show sample results\n",
    "print(\"\\n[Sample Generation Results]\")\n",
    "for i, result in enumerate(final_results[:3]):\n",
    "    print(f\"\\n--- Sample {i+1} ({result['category']}) ---\")\n",
    "    print(f\"Query: {result['query'][:100]}...\")\n",
    "    print(f\"Generated: {result['generated'][:300]}...\")\n",
    "\n",
    "# Finish WandB\n",
    "wandb.finish()\n",
    "print(\"\\n[WandB run finished]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SKD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
